{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of Mel-Spectrogram bands to extract.\n",
    "n_mels = 128\n",
    "\n",
    "# Initialize empty arrays to hold the feature matrix from audio and corresponding labels.\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Determine the maximum time length\n",
    "max_time_length = 0\n",
    "for file in sorted(os.listdir(data_path)):\n",
    "    if file.endswith(\".wav\"):\n",
    "        audio_data, sample_rate = librosa.load(os.path.join(data_path, file))\n",
    "        spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_mels=n_mels)\n",
    "        max_time_length = max(max_time_length, spectrogram.shape[1])\n",
    "\n",
    "# Iterate through all of the .wav files in the training directory.\n",
    "for file in sorted(os.listdir(data_path)):\n",
    "    if file.endswith(\".wav\"):\n",
    "        # Load the audio file using librosa and our files in Google Drive.\n",
    "        audio_data, sample_rate = librosa.load(os.path.join(data_path, file))\n",
    "\n",
    "        # Determine the Mel-Spectrogram features from the audio data provided above.\n",
    "        spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_mels=n_mels)\n",
    "\n",
    "        # Pad the spectrogram to have the same time length\n",
    "        pad_width = max_time_length - spectrogram.shape[1]\n",
    "        spectrogram_padded = np.pad(spectrogram, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "        # Add the Mel-Spectrogram features to the data matrix\n",
    "        X_train.append(spectrogram_padded)\n",
    "\n",
    "        # Determine the emotion label of the audio file based on its filename.\n",
    "        label = \"\"\n",
    "        idx = 0\n",
    "        while not (file[idx].isdigit()):\n",
    "            label += file[idx]\n",
    "            idx += 1\n",
    "        y_train.append(label)\n",
    "\n",
    "# Convert our data into np.arrays and expand the dimensions\n",
    "X_train = np.array(X_train)[:, :, :, np.newaxis]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded)\n",
    "\n",
    "# Create the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(n_mels, max_time_length, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "X_train, y_train,\n",
    "validation_data=(X_val, y_val),\n",
    "epochs=40,\n",
    "batch_size=32,\n",
    "callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = tf.keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "# Prepare the test data\n",
    "file_names = []\n",
    "X_test = []\n",
    "\n",
    "for file in sorted(os.listdir(test_path)):\n",
    "    if file.endswith(\".wav\"):\n",
    "        file_names.append(file.split(\".\")[0])\n",
    "        audio_data, sample_rate = librosa.load(os.path.join(test_path, file))\n",
    "        spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_mels=n_mels)\n",
    "        spectrogram_padded = np.pad(spectrogram, ((0, 0), (0, max_time_length - spectrogram.shape[1])), mode='constant')\n",
    "        X_test.append(spectrogram_padded)\n",
    "\n",
    "X_test = np.array(X_test)[:, :, :, np.newaxis]\n",
    "\n",
    "# Make predictions\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Convert the integer labels back into emotion strings\n",
    "y_pred_emotions = encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Prepare the predictions for saving to a CSV file\n",
    "predictions = np.column_stack((file_names, y_pred_emotions))\n",
    "\n",
    "# Sort the predictions array by the first column (the sorted file names)\n",
    "predictions = predictions[np.lexsort((predictions[:, 0],))]\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "np.savetxt(\"predictions.csv\", predictions, fmt='%s', delimiter=\",\", encoding='utf-8')\n",
    "with open(\"predictions.csv\", \"r+\") as f:\n",
    "    content = f.read()\n",
    "    f.seek(0, 0)\n",
    "    f.write(\"filename,Label\\n\" + content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
